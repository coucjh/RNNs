{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNMD5M6mQ2YY"
      },
      "source": [
        "# Coursework 3: RNNs\n",
        "\n",
        "#### Instructions\n",
        "\n",
        "Please submit on CATe a zip file named *CW3_RNNs.zip* containing a version of this notebook with your answers. Write your answers in the cells below for each question.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKehhGDF-Qte"
      },
      "source": [
        "## Recurrent models coursework\n",
        "\n",
        "This coursework is separated into a coding and a theory component.\n",
        "\n",
        "For the first part, you will use the Google Speech Commands v0.02 subset that you used in the RNN tutorial: http://www.doc.ic.ac.uk/~pam213/co460_files/ \n",
        "\n",
        "### Part 1 - Coding\n",
        "In this part you will have to:\n",
        "\n",
        "- Implement an LSTM\n",
        "- Implement a GRU\n",
        "\n",
        "### Part 2 - Theory\n",
        "\n",
        "Here you will answer some theoretical questions about RNNs -- no detailed proofs and no programming."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaI4P8SZ-U2j"
      },
      "source": [
        "### Part 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWGb-eUeXtex"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "We will be using the Google [*Speech Commands*](https://www.tensorflow.org/tutorials/sequences/audio_recognition) v0.02 [1] dataset.\n",
        "\n",
        "[1] Warden, P. (2018). [Speech commands: A dataset for limited-vocabulary speech recognition](https://arxiv.org/abs/1804.03209). *arXiv preprint arXiv:1804.03209.*"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive\n",
        "\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "%load_ext google.colab.data_table\n",
        "content_path = '/content/drive/MyDrive/DL_cw/'\n",
        "data_path = './data/'\n",
        "drive.mount('/content/drive/')\n",
        "content_path = Path(content_path)\n",
        "\n",
        "%cd '/content/drive/MyDrive/DL_cw/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hiu-Sla9smCa",
        "outputId": "b6e413fb-bc11-4c1f-9ec8-fa0cb1ee26c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive/DL_cw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Of3R4B-SsPwT",
        "outputId": "c7648669-470a-4ffb-c4a2-78d69605b775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/DL_cw/\n"
          ]
        }
      ],
      "source": [
        "## MAKE SURE THIS POINTS INSIDE THE DATASET FOLDER.\n",
        "dataset_folder = str(content_path) + '/' # this should change depending on where you have stored the data files\n",
        "print(dataset_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zd50Q71ssPxN"
      },
      "source": [
        "### Initial code before coursework questions start:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qt3KzJzBPdHU"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from scipy.io.wavfile import read\n",
        "import librosa\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False\n",
        "\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTeJi8aksPxU"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed_value):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKSnqpAJLVwx"
      },
      "outputs": [],
      "source": [
        "class SpeechCommandsDataset(Dataset):\n",
        "    \"\"\"Google Speech Commands dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, split):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            root_dir (string): Directory with all the data files.\n",
        "            split    (string): In [\"train\", \"valid\", \"test\"].\n",
        "        \"\"\"\n",
        "        self.root_dir = str(root_dir)\n",
        "        self.split = split\n",
        "\n",
        "        self.number_of_classes = len(self.get_classes())\n",
        "\n",
        "        self.class_to_file = defaultdict(list)\n",
        "\n",
        "        self.valid_filenames = self.get_valid_filenames()\n",
        "        self.test_filenames = self.get_test_filenames()\n",
        "\n",
        "        for c in self.get_classes():\n",
        "            file_name_list = sorted(os.listdir(self.root_dir + \"data_speech_commands_v0.02/\" + c))\n",
        "            for filename in file_name_list:\n",
        "                if split == \"train\":\n",
        "                    if (filename not in self.valid_filenames[c]) and (filename not in self.test_filenames[c]):\n",
        "                        self.class_to_file[c].append(filename)\n",
        "                elif split == \"valid\":\n",
        "                    if filename in self.valid_filenames[c]:\n",
        "                        self.class_to_file[c].append(filename)\n",
        "                elif split == \"test\":\n",
        "                    if filename in self.test_filenames[c]:\n",
        "                        self.class_to_file[c].append(filename)\n",
        "                else:\n",
        "                    raise ValueError(\"Invalid split name.\")\n",
        "\n",
        "        self.filepath_list = list()\n",
        "        self.label_list = list()\n",
        "        for cc, c in enumerate(self.get_classes()):\n",
        "            f_extension = sorted(list(self.class_to_file[c]))\n",
        "            l_extension = [cc for i in f_extension]\n",
        "            f_extension = [self.root_dir + \"data_speech_commands_v0.02/\" + c + \"/\" + filename for filename in f_extension]\n",
        "            self.filepath_list.extend(f_extension)\n",
        "            self.label_list.extend(l_extension)\n",
        "        self.number_of_samples = len(self.filepath_list)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.number_of_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = np.zeros((16000, ), dtype=np.float32)\n",
        "\n",
        "        sample_file = self.filepath_list[idx]\n",
        "\n",
        "        sample_from_file = read(sample_file)[1]\n",
        "        sample[:sample_from_file.size] = sample_from_file\n",
        "        sample = sample.reshape((16000, ))\n",
        "        \n",
        "        sample = librosa.feature.mfcc(y=sample, sr=16000, hop_length=512, n_fft=2048).transpose().astype(np.float32)\n",
        "\n",
        "        label = self.label_list[idx]\n",
        "\n",
        "        return sample, label\n",
        "\n",
        "    def get_classes(self):\n",
        "        return ['one', 'two', 'three']\n",
        "\n",
        "    def get_valid_filenames(self):\n",
        "        class_names = self.get_classes()\n",
        "\n",
        "        class_to_filename = defaultdict(set)\n",
        "        with open(self.root_dir + \"data_speech_commands_v0.02/validation_list.txt\", \"r\") as fp:\n",
        "            for line in fp:\n",
        "                clean_line = line.strip().split(\"/\")\n",
        "\n",
        "                if clean_line[0] in class_names:\n",
        "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
        "\n",
        "        return class_to_filename\n",
        "\n",
        "    def get_test_filenames(self):\n",
        "        class_names = self.get_classes()\n",
        "\n",
        "        class_to_filename = defaultdict(set)\n",
        "        with open(self.root_dir + \"data_speech_commands_v0.02/testing_list.txt\", \"r\") as fp:\n",
        "            for line in fp:\n",
        "                clean_line = line.strip().split(\"/\")\n",
        "\n",
        "                if clean_line[0] in class_names:\n",
        "                    class_to_filename[clean_line[0]].add(clean_line[1])\n",
        "\n",
        "        return class_to_filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vx8ptirGKa9u"
      },
      "outputs": [],
      "source": [
        "train_dataset = SpeechCommandsDataset(dataset_folder,\n",
        "                                      \"train\")\n",
        "valid_dataset = SpeechCommandsDataset(dataset_folder,\n",
        "                                      \"valid\")\n",
        "\n",
        "test_dataset = SpeechCommandsDataset(dataset_folder,\n",
        "                                     \"test\")\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "\n",
        "num_epochs = 5\n",
        "valid_every_n_steps = 20\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(dataset=valid_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=False)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset.get_classes()\n",
        "train_dataset.__getitem__(100)[0][0].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoP06ucz04Ye",
        "outputId": "2c2f5f30-0337-48e5-e93d-0a8031c03d72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJwOesOQOSh9"
      },
      "source": [
        "### Question 1:  Finalise the LSTM and GRU cells by completing the missing code\n",
        "\n",
        "You are allowed to use nn.Linear."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQu9Yxfy-Wqj"
      },
      "outputs": [],
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        \n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 1a) Complete the missing code\n",
        "        ########################################################################\n",
        "        # forget gate layers\n",
        "        self.forget_u1 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.forget_v1 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.sigmoid_forget = nn.Sigmoid()\n",
        "\n",
        "        # input gate layers\n",
        "        self.input_gate_u2 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.input_gate_v2 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.sigmoid_gate = nn.Sigmoid()\n",
        "\n",
        "        # cell memory layers\n",
        "        self.mem_gate_u3 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.mem_gate_v3 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.activation_gate = nn.Tanh()\n",
        "\n",
        "        # out gate layers\n",
        "        self.out_gate_u4 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.out_gate_v4 = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n",
        "        self.sigmoid_hidden_out = nn.Sigmoid()\n",
        "\n",
        "        self.activation_final = nn.Tanh()\n",
        "\n",
        "        self.reset_parameters()\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 1b) Complete the missing code\n",
        "        ########################################################################\n",
        "    def forget_gate(self, x, h):\n",
        "        x = self.forget_u1(x)\n",
        "        h = self.forget_v1(h)\n",
        "\n",
        "        return self.sigmoid_forget(x + h)\n",
        "\n",
        "\n",
        "    def input_gate(self, x, h):\n",
        "        x_temp = self.input_gate_u2(x)\n",
        "        h_temp = self.input_gate_v2(h)\n",
        "\n",
        "        return self.sigmoid_gate(x_temp + h_temp)\n",
        "    \n",
        "\n",
        "    def cell_memory_gate(self, i, f, x, h, c_prev):\n",
        "        x = self.mem_gate_u3(x)\n",
        "        h = self.mem_gate_v3(h)\n",
        "\n",
        "        k = self.activation_gate(x + h)\n",
        "        g = k * i\n",
        "        \n",
        "        c = f * c_prev\n",
        "        c_next = g + c\n",
        "\n",
        "        return c_next\n",
        "\n",
        "\n",
        "    def out_gate(self, x, h):\n",
        "        x = self.out_gate_u4(x)\n",
        "        h = self.out_gate_v4(h)\n",
        "\n",
        "        return self.sigmoid_hidden_out(x + h)\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        if hx is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "            hx = (hx, hx)\n",
        "            \n",
        "        # We used hx to pack both the hidden and cell states\n",
        "        hx, cx = hx\n",
        "\n",
        "        i = self.input_gate(input, hx) # Pass through input gate \n",
        "        f = self.forget_gate(input, hx) # Pass through forget gate\n",
        "        cy = self.cell_memory_gate(i, f, input, hx ,cx) # Pass through cell memory gate\n",
        "        o = self.out_gate(input, hx) # Pass through out gate\n",
        "        hy = o * self.activation_final(cy) # Final activation\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "\n",
        "        return (hy, cy)\n",
        "\n",
        "class BasicRNNCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True, nonlinearity=\"tanh\"):\n",
        "        super(BasicRNNCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "        self.nonlinearity = nonlinearity\n",
        "        if self.nonlinearity not in [\"tanh\", \"relu\"]:\n",
        "            raise ValueError(\"Invalid nonlinearity selected for RNN.\")\n",
        "\n",
        "        self.x2h = nn.Linear(input_size, hidden_size, bias=bias)\n",
        "        self.h2h = nn.Linear(hidden_size, hidden_size, bias=bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "        \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "            \n",
        "    def forward(self, input, hx=None):\n",
        "        if hx is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "\n",
        "        activation = getattr(nn.functional, self.nonlinearity)\n",
        "        hy = activation(self.x2h(input) + self.h2h(hx))\n",
        "\n",
        "        return hy\n",
        "    \n",
        "    \n",
        "class GRUCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, bias=True):\n",
        "        super(GRUCell, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bias = bias\n",
        "\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 1c) Complete the missing code\n",
        "        ########################################################################\n",
        "        # reset gate layers\n",
        "        self.reset_gate_u1 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.reset_gate_v1 = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n",
        "\n",
        "        self.reset_gate_u2 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.reset_gate_v2 = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n",
        "        self.activation_1 = nn.Sigmoid()\n",
        "\n",
        "        # update gate layers\n",
        "        self.mem_gate_u3 = nn.Linear(self.input_size, self.hidden_size, bias=bias)\n",
        "        self.mem_gate_v3 = nn.Linear(self.hidden_size, self.hidden_size, bias=bias)\n",
        "        self.activation_2 = nn.Sigmoid()\n",
        "\n",
        "        self.activation_3 = nn.Tanh()\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "        self.reset_parameters()\n",
        "        \n",
        "\n",
        "    def reset_parameters(self):\n",
        "        std = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for w in self.parameters():\n",
        "            w.data.uniform_(-std, std)\n",
        "\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 1d) Complete the missing code\n",
        "        ########################################################################\n",
        "        x = input\n",
        "\n",
        "    def reset_gate(self, x, h):\n",
        "        x_1 = self.reset_gate_u1(x)\n",
        "        h_1 = self.reset_gate_v1(h)\n",
        "        reset = self.activation_1(x_1 + h_1)\n",
        "\n",
        "        return reset\n",
        "\n",
        "\n",
        "    def update_gate(self, x, h):\n",
        "        x_2 = self.reset_gate_u2(x)\n",
        "        h_2 = self.reset_gate_v2(h)\n",
        "        z = self.activation_2(h_2 + x_2)\n",
        "\n",
        "        return z\n",
        "\n",
        "\n",
        "    def update_component(self, x,h,r):\n",
        "        x_3 = self.mem_gate_u3(x)\n",
        "        h_3 = r * self.mem_gate_v3(h) \n",
        "        gate_update = self.activation_3(x_3 + h_3)\n",
        "\n",
        "        return gate_update\n",
        "\n",
        "\n",
        "    def forward(self, input, hx=None):\n",
        "        if hx is None:\n",
        "            hx = input.new_zeros(input.size(0), self.hidden_size, requires_grad=False)\n",
        "\n",
        "        r = self.reset_gate(input, hx) # Pass through reset gate\n",
        "        z = self.update_gate(input, hx) # Pass through update gate\n",
        "        n = self.update_component(input, hx, r) # Pass through almost update component\n",
        "        hy = (1 - z) * n  + z * hx\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "        \n",
        "        return hy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLZIprRusPxg"
      },
      "source": [
        "### Question 2:  Finalise the RNNModel and BidirRecurrentModel\n",
        "\n",
        "Note that there are serveral different ways that one can implement a bi-directional recurrent neural network. Specifically in this coursework we ask for implementation of the following type of architecture (with e.g. 2 layers for each direction as an example, your implementation should work for any number of layers):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "SU61hexBsPxg",
        "outputId": "ee8c58b2-6108-4ba0-e12b-270ebf7639ea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-a8abd357b500>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bidirectional_rnn_arch.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconfined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munconfined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, url, filename)\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0municode_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1043\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretina\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retina_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/display.py\u001b[0m in \u001b[0;36mreload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0;34m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_flags\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bidirectional_rnn_arch.png'"
          ]
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(filename='bidirectional_rnn_arch.png', width=300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-jqDWEAsPxl"
      },
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, mode, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.rnn_cell_list = nn.ModuleList()\n",
        "        \n",
        "        if mode == 'LSTM':\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 2a) Complete the missing code\n",
        "        #  Append the appropriate LSTM cells to rnn_cell_list\n",
        "        ########################################################################\n",
        "            self.rnn_cell_list.append(LSTMCell(input_size, hidden_size))\n",
        "            \n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(LSTMCell(hidden_size, \n",
        "                                                       hidden_size))\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "\n",
        "        elif mode == 'GRU':\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 2b) Complete the missing code\n",
        "        #  Append the appropriate GRU cells to rnn_cell_list\n",
        "        ########################################################################\n",
        "            self.rnn_cell_list.append(GRUCell(input_size, hidden_size))\n",
        "            \n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(GRUCell(hidden_size, hidden_size))\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ######################################################################## \n",
        "        \n",
        "        elif mode == 'RNN_TANH':\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 2c) Complete the missing code\n",
        "        #  Append the appropriate RNN cells to rnn_cell_list\n",
        "        ########################################################################\n",
        "            self.rnn_cell_list.append(BasicRNNCell(input_size, hidden_size))\n",
        "            \n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(BasicRNNCell(hidden_size, \n",
        "                                                           hidden_size))\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "\n",
        "        elif mode == 'RNN_RELU':\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 2d) Complete the missing code\n",
        "        #  Append the appropriate RNN cells to rnn_cell_list\n",
        "        ########################################################################\n",
        "            self.rnn_cell_list.append(BasicRNNCell(input_size, \n",
        "                                                   hidden_size, \n",
        "                                                   nonlinearity='relu'))\n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(BasicRNNCell(hidden_size, \n",
        "                                                           hidden_size, \n",
        "                                                           nonlinearity='relu'))\n",
        "        ########################################################################\n",
        "        ## END OF YOUR CODE\n",
        "        ########################################################################\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid RNN mode selected.\")\n",
        "\n",
        "        self.att_fc = nn.Linear(self.hidden_size, 1)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "        \n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        h0 = [None] * self.num_layers if hx is None else list(hx)\n",
        "        \n",
        "        # In this forward pass we want to create our RNN from the rnn cells,\n",
        "        # ..taking the hidden states from the final RNN layer and passing these \n",
        "        # ..through our fully connected layer (fc).\n",
        "        \n",
        "        # The multi-layered RNN should be able to run when the mode is either \n",
        "        # .. LSTM, GRU, RNN_TANH or RNN_RELU.\n",
        "\n",
        "        ########################################################################    \n",
        "        ## START OF YOUR CODE - Question 2e) Complete the missing code  \n",
        "        ########################################################################\n",
        "\n",
        "        # Rearrange to (sequence_dim X batch_size X input_dim)\n",
        "        X = list(input.permute(1, 0, 2))\n",
        "\n",
        "        sd = input.shape[1] # Sequence dimension\n",
        "\n",
        "        # Loops through cells then through sequence\n",
        "        for k, cell in enumerate(self.rnn_cell_list):\n",
        "            for i in range(sd):\n",
        "\n",
        "                # If first i, set hidden state input to None\n",
        "                if i == 0:\n",
        "                    hx = h0[k]\n",
        "\n",
        "                # Take i'th input in sequence as input with hx from previous \n",
        "                # cell and pass through network\n",
        "                hx_minus_one = X[i]\n",
        "                hx = cell(hx_minus_one, hx)\n",
        "                \n",
        "                # Take first element of tuple if LSTM\n",
        "                if self.mode != \"LSTM\":\n",
        "                    X[i] = hx\n",
        "                else:\n",
        "                    X[i] = hx[0]\n",
        "\n",
        "        outs = X\n",
        "        ########################################################################    \n",
        "        ## END OF YOUR CODE \n",
        "        ########################################################################\n",
        "\n",
        "        out = outs[-1].squeeze()\n",
        "\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "\n",
        "class BidirRecurrentModel(nn.Module):\n",
        "    def __init__(self, mode, input_size, hidden_size, num_layers, bias, output_size):\n",
        "        super(BidirRecurrentModel, self).__init__()\n",
        "        self.mode = mode\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bias = bias\n",
        "        self.output_size = output_size\n",
        "        \n",
        "        self.rnn_cell_list = nn.ModuleList()\n",
        "        self.rnn_cell_list_rev = nn.ModuleList()\n",
        "\n",
        "        # THIS HAS BEEN CHANGED TO ACCOMODATE THE CONCATATENATED OUTPUTS\n",
        "        self.fc = nn.Linear(self.hidden_size*2, self.output_size) \n",
        "\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE - Question 2f) Complete the missing code\n",
        "        ########################################################################\n",
        "        if mode == 'LSTM':\n",
        "        #  Append the appropriate LSTM cells to rnn_cell_list\n",
        "            self.rnn_cell_list.append(LSTMCell(input_size, hidden_size))\n",
        "            self.rnn_cell_list_rev.append(LSTMCell(input_size, hidden_size))\n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(LSTMCell(hidden_size, \n",
        "                                                       hidden_size))\n",
        "                    self.rnn_cell_list_rev.append(LSTMCell(hidden_size, \n",
        "                                                       hidden_size))\n",
        "\n",
        "        elif mode == 'GRU':\n",
        "        #  Append the appropriate GRU cells to rnn_cell_list\n",
        "            self.rnn_cell_list.append(GRUCell(input_size, hidden_size))\n",
        "            self.rnn_cell_list_rev.append(GRUCell(input_size, hidden_size))\n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(GRUCell(hidden_size, hidden_size))\n",
        "                    self.rnn_cell_list_rev.append(GRUCell(hidden_size, hidden_size))\n",
        "        \n",
        "        elif mode == 'RNN_TANH':\n",
        "        #  Append the appropriate RNN cells to rnn_cell_list\n",
        "            self.rnn_cell_list.append(BasicRNNCell(input_size, hidden_size))\n",
        "            self.rnn_cell_list_rev.append(BasicRNNCell(input_size, hidden_size))\n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(BasicRNNCell(hidden_size, \n",
        "                                                           hidden_size))\n",
        "                    self.rnn_cell_list_rev.append(BasicRNNCell(hidden_size, \n",
        "                                                           hidden_size))\n",
        "                \n",
        "        elif mode == 'RNN_RELU':\n",
        "        #  Append the appropriate RNN cells to rnn_cell_list\n",
        "            self.rnn_cell_list.append(BasicRNNCell(input_size, \n",
        "                                                   hidden_size, \n",
        "                                                   nonlinearity='relu'))\n",
        "            self.rnn_cell_list_rev.append(BasicRNNCell(input_size, \n",
        "                                                       hidden_size,\n",
        "                                                       nonlinearity='relu'))\n",
        "\n",
        "            if num_layers > 1:\n",
        "                for layer in range(num_layers - 1):\n",
        "                    self.rnn_cell_list.append(BasicRNNCell(hidden_size, \n",
        "                                                           hidden_size, \n",
        "                                                           nonlinearity='relu'))\n",
        "                    self.rnn_cell_list_rev.append(BasicRNNCell(hidden_size, \n",
        "                                                           hidden_size, \n",
        "                                                           nonlinearity='relu'))\n",
        "        ########################################################################    \n",
        "        ## END OF YOUR CODE \n",
        "        ########################################################################\n",
        "        \n",
        "    def forward(self, input, hx=None):\n",
        "\n",
        "        h0 = [None] * self.num_layers if hx is None else list(hx)\n",
        "        \n",
        "        # In this forward pass we want to create our Bidirectional RNN from the rnn cells,\n",
        "        # .. taking the hidden states from the final RNN layer with their reversed counterparts\n",
        "        # .. before concatening these and running them through the fully connected layer (fc)\n",
        "        \n",
        "        # The multi-layered RNN should be able to run when the mode is either \n",
        "        # .. LSTM, GRU, RNN_TANH or RNN_RELU.\n",
        "\n",
        "        ########################################################################\n",
        "        ## START OF YOUR CODE  - Question 2g) Complete the missing code\n",
        "        ########################################################################\n",
        "\n",
        "        # Rearrange to (sequence_dim X batch_size X input_dim)\n",
        "        X = list(input.permute(1, 0, 2)) \n",
        "        X_rev = X.copy() # Use same data but pass backwards through\n",
        "\n",
        "        sd = input.shape[1] # Sequence dimension\n",
        "\n",
        "        # FORWARD\n",
        "        # Loops through cells then through sequence\n",
        "        for k, cell in enumerate(self.rnn_cell_list):\n",
        "            for i in range(sd):\n",
        "                \n",
        "                # If first i, set hidden state input to None\n",
        "                if i == 0:\n",
        "                    hx = h0[k]\n",
        "\n",
        "                # Take i'th input in sequence as input with hx from previous \n",
        "                # cell and pass through network\n",
        "                hx_minus_one = X[i]\n",
        "                hx = cell(hx_minus_one, hx)\n",
        "                \n",
        "                # Take first element of tuple if LSTM\n",
        "                if self.mode != \"LSTM\":\n",
        "                    X[i] = hx\n",
        "                else:\n",
        "                    X[i] = hx[0]\n",
        "        outs = X\n",
        "\n",
        "        # REVERSE\n",
        "        for k, cell in enumerate(self.rnn_cell_list_rev):\n",
        "            for i in range(sd):\n",
        "\n",
        "                if i == 0:\n",
        "                    hx = h0[k]\n",
        "\n",
        "                hx_minus_one = X_rev[-(i+1)]\n",
        "                hx = cell(hx_minus_one, hx)\n",
        "                \n",
        "                # Update inputs\n",
        "                if self.mode != \"LSTM\":\n",
        "                    X_rev[-(i+1)] = hx\n",
        "                else:\n",
        "                    X_rev[-(i+1)] = hx[0]\n",
        "\n",
        "        outs_rev = X_rev\n",
        "        ########################################################################    \n",
        "        ## END OF YOUR CODE \n",
        "        ########################################################################\n",
        "\n",
        "        out = outs[-1].squeeze()\n",
        "        out_rev = outs_rev[0].squeeze()\n",
        "        out = torch.cat((out, out_rev), 1)\n",
        "\n",
        "        out = self.fc(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VuFIcmQsPxm"
      },
      "source": [
        "The code below trains a network based on your code above. This should work without error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Oxz6enSsPxm",
        "outputId": "8b52bbfa-f389-47fc-95aa-660a77569e0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 20. Loss: 1.2095831632614136. V-Accuracy: 35  T-Accuracy: 35\n",
            "Iteration: 40. Loss: 1.1152595281600952. V-Accuracy: 38  T-Accuracy: 37\n",
            "Iteration: 60. Loss: 1.0666297674179077. V-Accuracy: 38  T-Accuracy: 36\n",
            "Iteration: 80. Loss: 1.0519579648971558. V-Accuracy: 39  T-Accuracy: 38\n",
            "epoch: 1\n",
            "Iteration: 100. Loss: 1.0632435083389282. V-Accuracy: 41  T-Accuracy: 40\n",
            "Iteration: 120. Loss: 1.0557938814163208. V-Accuracy: 37  T-Accuracy: 40\n",
            "Iteration: 140. Loss: 1.059493899345398. V-Accuracy: 37  T-Accuracy: 40\n",
            "Iteration: 160. Loss: 1.1052502393722534. V-Accuracy: 36  T-Accuracy: 40\n",
            "Iteration: 180. Loss: 1.0554403066635132. V-Accuracy: 39  T-Accuracy: 40\n",
            "epoch: 2\n",
            "Iteration: 200. Loss: 1.0627682209014893. V-Accuracy: 43  T-Accuracy: 41\n",
            "Iteration: 220. Loss: 1.1014127731323242. V-Accuracy: 40  T-Accuracy: 41\n",
            "Iteration: 240. Loss: 1.1135965585708618. V-Accuracy: 39  T-Accuracy: 41\n",
            "Iteration: 260. Loss: 1.0812163352966309. V-Accuracy: 43  T-Accuracy: 41\n",
            "epoch: 3\n",
            "Iteration: 280. Loss: 1.0812065601348877. V-Accuracy: 43  T-Accuracy: 41\n",
            "Iteration: 300. Loss: 1.0759888887405396. V-Accuracy: 41  T-Accuracy: 41\n",
            "Iteration: 320. Loss: 1.0153034925460815. V-Accuracy: 43  T-Accuracy: 42\n",
            "Iteration: 340. Loss: 1.0693275928497314. V-Accuracy: 44  T-Accuracy: 43\n",
            "Iteration: 360. Loss: 1.0514243841171265. V-Accuracy: 45  T-Accuracy: 44\n",
            "epoch: 4\n",
            "Iteration: 380. Loss: 1.0222086906433105. V-Accuracy: 41  T-Accuracy: 44\n",
            "Iteration: 400. Loss: 0.9883381128311157. V-Accuracy: 46  T-Accuracy: 46\n",
            "Iteration: 420. Loss: 1.0229122638702393. V-Accuracy: 52  T-Accuracy: 52\n",
            "Iteration: 440. Loss: 1.0645238161087036. V-Accuracy: 46  T-Accuracy: 52\n",
            "Iteration: 460. Loss: 0.8303520083427429. V-Accuracy: 61  T-Accuracy: 61\n"
          ]
        }
      ],
      "source": [
        "seq_dim, input_dim = train_dataset[0][0].shape\n",
        "output_dim = 3\n",
        "\n",
        "hidden_dim = 32\n",
        "layer_dim = 2\n",
        "bias = True\n",
        "\n",
        "### Change the code below to try running different models:\n",
        "model = RNNModel(\"RNN_RELU\", input_dim, hidden_dim, layer_dim, bias, output_dim)\n",
        "# model = BidirRecurrentModel(\"GRU\", input_dim, hidden_dim, layer_dim, bias, output_dim)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "learning_rate = 0.01\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_list = []\n",
        "iter = 0\n",
        "max_v_accuracy = 0\n",
        "reported_t_accuracy = 0\n",
        "max_t_accuracy = 0\n",
        "for epoch in range(num_epochs):\n",
        "    print(f'epoch: {epoch}')\n",
        "    for i, (audio, labels) in enumerate(train_loader):\n",
        "        # print(f'iteration: {i}')\n",
        "        if torch.cuda.is_available():\n",
        "            audio = Variable(audio.view(-1, seq_dim, input_dim).cuda())\n",
        "            labels = Variable(labels.cuda())\n",
        "        else:\n",
        "            audio = Variable(audio.view(-1, seq_dim, input_dim))\n",
        "            labels = Variable(labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(audio)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            loss.cuda()\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_list.append(loss.item())\n",
        "        iter += 1\n",
        "\n",
        "        if iter % valid_every_n_steps == 0:\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for audio, labels in valid_loader:\n",
        "                if torch.cuda.is_available():\n",
        "                    audio = Variable(audio.view(-1, seq_dim, input_dim).cuda())\n",
        "                else:\n",
        "                    audio = Variable(audio.view(-1, seq_dim, input_dim))\n",
        "\n",
        "                outputs = model(audio)\n",
        "\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                total += labels.size(0)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "                else:\n",
        "                    correct += (predicted == labels).sum()\n",
        "\n",
        "            v_accuracy = 100 * correct // total\n",
        "            \n",
        "            is_best = False\n",
        "            if v_accuracy >= max_v_accuracy:\n",
        "                max_v_accuracy = v_accuracy\n",
        "                is_best = True\n",
        "\n",
        "            if is_best:\n",
        "                for audio, labels in test_loader:\n",
        "                    if torch.cuda.is_available():\n",
        "                        audio = Variable(audio.view(-1, seq_dim, input_dim).cuda())\n",
        "                    else:\n",
        "                        audio = Variable(audio.view(-1, seq_dim, input_dim))\n",
        "\n",
        "                    outputs = model(audio)\n",
        "\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                    total += labels.size(0)\n",
        "\n",
        "                    if torch.cuda.is_available():\n",
        "                        correct += (predicted.cpu() == labels.cpu()).sum()\n",
        "                    else:\n",
        "                        correct += (predicted == labels).sum()\n",
        "\n",
        "                t_accuracy = 100 * correct // total\n",
        "                reported_t_accuracy = t_accuracy\n",
        "\n",
        "            print('Iteration: {}. Loss: {}. V-Accuracy: {}  T-Accuracy: {}'.format(iter, loss.item(), v_accuracy, reported_t_accuracy))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H6oJyOX-W7n"
      },
      "source": [
        "## Part 2: Theoretical questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CK0guD_b-ZAR"
      },
      "source": [
        "#### Theory question 1: \n",
        "What is the _vanishing gradients problem_ and why does it occur? Which activation functions are more or less impacted by this, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha2y337Li6b4"
      },
      "source": [
        "#### Your answers:\n",
        "* Your answer here describing vanishing gradients problem\n",
        "* Two examples of activation functions more impacted by vanishing gradients\n",
        "* Two examples of activation functions less impacted by vanishing gradients, why are they impacted less?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the RNN, information travels forward through time in the network via a series of cells to the output neurons, the information from previous time steps is then used for all the next time points. The cost function and then error is calculated at each time point. Instead of backpropagating errors through a single feedforward network, the errors are propagated through all the time points and then backpropagated through time via the cells. Between each time step we have to perform a weight matrix multiplication. This involves many factors of this weight matrix and repeated computation of the gradients with respect to this weight matrix. If we have many values in this chain of matrix multiplications where the gradient values or weight values are less than one, then these gradients become increasingly smaller as they multiply, eventually 'vanishing' whereby we cannot effectively train the network. This causes errors due to further back time steps having increasingly smaller gradients. It can also mean that the model places a bias on parameters to capture short term dependencies in a sequence.\n",
        "\n",
        "- The two main examples of activation functions that are impacted by this effect are the tanh and sigmoid functions. This is because they 'saturate' at between 0 and 1 for sigmoid and between -1 and 1 for tanh. In both cases the derivatives become extremely close to 0, and then the products of these gradients disappear to nothing.\n",
        "\n",
        "- Two functions that mitigate this effect are ReLU and leaky ReLU. Both of these have gradient 1 when input > 0 and mean that taking the product of many of these combined have the useful property of being either 1 or 0, and there is no diminishing as before. Leaky ReLU has a very small gradient for negative values instead of a flat slope as for ReLU. This means that gradients less than zero dont completely reduce to zero, solving the 'dying' ReLU problem when gradients on the left hand side of x = 0 are saturated."
      ],
      "metadata": {
        "id": "lw5kQY7WkyTN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YG1qI0pjsPxn"
      },
      "source": [
        "#### Theory question 2: \n",
        "Why do LSTMs help address the vanishing gradient problem compared to a vanilla RNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTMs are a type of gated cell. This is a more complex recurrent unit that uses gates to control what information is passed through in place of the basic RNN cell. This controls what information passes through the cell by tracking information throughout many time steps. The cell now contains a series of standard neural network operations (sigmoid and tanh) and point wise matrix multiplications combined to form gates. Three gates exist with different functions:\n",
        "\n",
        "- forget gate forgets irrelevent part of the previous state\n",
        "- store gate stores relevent new information into the cell state\n",
        "- output gate controls what information is sent to the next time step\n",
        "\n",
        "The LSTM also maintains a seperate value of the cell state (c_t) as well as output h_t. The combination of these components mean the cells can regulate the flow of information. The most important effect of this is that these gates result in the uninterupted flow of gradients across time by backpropagation through the cell states. The gradient computation is now taken wrt the c_t values only, meaning the gradients are less susceptible to the vanishing gradient effect. The individual outputs of the different gates are now summed as the cell state (c_t). This balances the c_t value across the different gates. This means that when gradients are taken, they are unlikely to be so small that the product after backpropagation through time vanishes."
      ],
      "metadata": {
        "id": "GSz0Pe7pqtXO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpefJuwe_c8o"
      },
      "source": [
        "#### Theory question 3: \n",
        "\n",
        "The plot below shows the training curves for three models A, B, and C, trained on the same dataset up to 100 epochs. The three models are a RNN, a LSTM and a GRU, not necessarily in that order.\n",
        "\n",
        "* Which could plausibly be which? Why? Please explain your reasoning.\n",
        "\n",
        "(In the cell below please set the values for A_model, B_model and C_model to be 'RNN', 'LSTM' or 'GRU'. This needs to be exact for the automatic marking.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBJJMuOUV-jJ",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "display(Image(filename='Performance by epoch.png', width=550))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYypU57tsPxt"
      },
      "outputs": [],
      "source": [
        "# Answers below:\n",
        "\n",
        "A_model = 'GRU'\n",
        "B_model = 'LSTM'\n",
        "C_model = 'RNN'\n",
        "\n",
        "# Give your reasons below:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The RNN curve (curve C) is separated from the other two as the performance levels out at a lower value after time. This is due to it not being able to recognise long term dependencies, as the explanation of vanishing gradient and information flow above has demonstrated. It will never be able to perform as well as the other gated cell techniques. It also demonstrates a higher performance more quickly. This is due to it having a fewer number of parameters with no gating, therefore it is able to train and update weights more quickly.\n",
        "The two remaining curves reach a similar level of performance but curve A demonstrates a higher performance sooner. This is likely to be as the GRU has two gates in comparison with the LSTM which has three. More gates means more parameters and longer time required to train.\n"
      ],
      "metadata": {
        "id": "dfZWy8Myxzgh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDVMb_UqsPx7"
      },
      "source": [
        "#### Theory question 4: \n",
        "\n",
        "When might you choose to use each of the three different types of models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuLS-d3LkOUR"
      },
      "source": [
        "#### Your answers:\n",
        "* Type of problem when best to use vanilla RNN:\n",
        "* Type of problem to use GRU:\n",
        "* Type of problem to use LSTM:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN1r5BqOkO7I"
      },
      "source": [
        "- Vanilla RNNs could be put to use for very short sequences in which the user is not concerned about the issue of vanishing gradient to pick up long term dependencies. It will always be outperformed by the other two but will use less computational effort and memory.\n",
        "- GRUs are simpler with two gates rather than three for the LSTM. This means they train faster as alluded to in the question above. They are known to perform better than LSTMs on less training data in some cases. They can also be considered more flexible to modify and so are a good general choice for language modelling.\n",
        "- LSTMs in theory remember longer sequence with the highest level of information regulation. For the largest datasets these should be the best option as they can pick up the longest term dependencies. This comes at the cost that they are computationally more expensive as well as using more memory than GRUs.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Coursework 3 - Full questions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}